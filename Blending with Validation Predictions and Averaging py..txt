import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import RidgeClassifier

# Split training data further into training and validation sets
X_train_part, X_val, y_train_part, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)

# Train individual base models on a portion of the training data
rf = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train_part, y_train_part)
et = ExtraTreesClassifier(n_estimators=100, random_state=42).fit(X_train_part, y_train_part)
xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42).fit(X_train_part, y_train_part)

# Generate predictions on the validation set
val_preds_rf = rf.predict_proba(X_val)[:, 1]
val_preds_et = et.predict_proba(X_val)[:, 1]
val_preds_xgb = xgb_model.predict_proba(X_val)[:, 1]

# Stack predictions as new features for meta-learner
stacked_val_preds = np.column_stack((val_preds_rf, val_preds_et, val_preds_xgb))

# Train meta-learner on validation predictions
meta_learner = RidgeClassifier()
meta_learner.fit(stacked_val_preds, y_val)

# Predict on test set by stacking base model predictions
test_preds_rf = rf.predict_proba(X_test)[:, 1]
test_preds_et = et.predict_proba(X_test)[:, 1]
test_preds_xgb = xgb_model.predict_proba(X_test)[:, 1]
stacked_test_preds = np.column_stack((test_preds_rf, test_preds_et, test_preds_xgb))

# Final predictions with meta-learner
final_preds = meta_learner.predict(stacked_test_preds)
print("Blending Accuracy:", accuracy_score(y_test, final_preds))
